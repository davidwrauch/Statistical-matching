I worked on an A/B test for a new feature in a website. The feature was available to 10% of the users of the website. The 10% who saw the feature could choose to engage with the new feature or not. The test population that used the feature had a lower conversion rate but a higher revenue per user rate. Leadership liked that the revenue per user was high and was going to move the test into production for 100% of the population, but I used a form of causal inference called statistical matching to find demographic matches of the people who clicked the feature and found that similar people who didn't click the new feature actually made us more money per user on average. Because of my analysis, we continued to improve the new feature as a test until the conversion rate eventually went up, and I estimate that we saved millions of dollars by waiting. Since this was an experiment, all the analysis was done in SQL to get the click-stream level data. The machine learning was performed in r, and the results were shown in Tableau. The results were shared in a weekly A/B testing results executive meeting with the CEO.
